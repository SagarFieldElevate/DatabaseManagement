{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycoingecko import CoinGeckoAPI\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from data_upload_utils import upload_to_github, update_airtable, create_airtable_record, delete_file_from_github\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize CoinGecko with retry mechanism\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "cg = CoinGeckoAPI(session=session)\n",
    "\n",
    "# Validate environment variables\n",
    "AIRTABLE_API_KEY = os.getenv(\"AIRTABLE_API_KEY\")\n",
    "GITHUB_TOKEN = os.getenv(\"GH_TOKEN\")\n",
    "if not all([AIRTABLE_API_KEY, GITHUB_TOKEN]):\n",
    "    logger.error(\"Missing environment variables: AIRTABLE_API_KEY or GITHUB_TOKEN\")\n",
    "    raise ValueError(\"Required environment variables are not set\")\n",
    "\n",
    "# Define coins\n",
    "coins = {\n",
    "    \"BTC\": \"bitcoin\",\n",
    "    \"ETH\": \"ethereum\",\n",
    "    \"ADA\": \"cardano\",\n",
    "    \"SOL\": \"solana\",\n",
    "    \"DOT\": \"polkadot\",\n",
    "    \"AVAX\": \"avalanche-2\",\n",
    "}\n",
    "\n",
    "# Airtable configuration\n",
    "BASE_ID = \"appnssPRD9yeYJJe5\"\n",
    "TABLE_NAME = \"Database\"\n",
    "AIRTABLE_URL = f\"https://api.airtable.com/v0/{BASE_ID}/{TABLE_NAME}\"\n",
    "AIRTABLE_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {AIRTABLE_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# GitHub configuration\n",
    "GITHUB_REPO = \"SagarFieldElevate/DatabaseManagement\"\n",
    "BRANCH = \"main\"\n",
    "UPLOAD_PATH = \"Uploads\"\n",
    "\n",
    "def fetch_coingecko_data():\n",
    "    \"\"\"Fetch 365 days of price data and compute metrics.\"\"\"\n",
    "    data = []\n",
    "    for symbol, coin_id in coins.items():\n",
    "        try:\n",
    "            market_data = cg.get_coin_market_chart_by_id(id=coin_id, vs_currency=\"usd\", days=365)\n",
    "            prices = market_data.get(\"prices\", [])\n",
    "            if not prices:\n",
    "                logger.warning(f\"No price data for {symbol}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(1, len(prices)):\n",
    "                prev_day = prices[i - 1]\n",
    "                current_day = prices[i]\n",
    "\n",
    "                prev_timestamp, prev_price = prev_day\n",
    "                current_timestamp, current_price = current_day\n",
    "\n",
    "                # Ensure prices are valid\n",
    "                if not (prev_price > 0 and current_price > 0):\n",
    "                    logger.warning(f\"Invalid price data for {symbol} at {current_timestamp}\")\n",
    "                    continue\n",
    "\n",
    "                high = max(prev_price, current_price)\n",
    "                low = min(prev_price, current_price)\n",
    "                volatility = ((high - low) / low) * 100\n",
    "                trading_range = high - low\n",
    "\n",
    "                data.append({\n",
    "                    \"symbol\": symbol,\n",
    "                    \"timestamp\": datetime.utcfromtimestamp(current_timestamp / 1000).isoformat(),\n",
    "                    \"high_24h_usd\": round(high, 2),\n",
    "                    \"low_24h_usd\": round(low, 2),\n",
    "                    \"volatility_24h_%\": round(volatility, 2),\n",
    "                    \"trading_range_24h_usd\": round(trading_range, 2),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch data for {symbol}: {str(e)}\")\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "def get_airtable_record():\n",
    "    \"\"\"Fetch Airtable record with filtering.\"\"\"\n",
    "    try:\n",
    "        params = {\"filterByFormula\": \"{Name}='365-Day Volatility and Range'\"}\n",
    "        response = requests.get(AIRTABLE_URL, headers=AIRTABLE_HEADERS, params=params)\n",
    "        response.raise_for_status()\n",
    "        records = response.json().get(\"records\", [])\n",
    "        if len(records) > 1:\n",
    "            logger.warning(\"Multiple records found with Name='365-Day Volatility and Range'. Using first.\")\n",
    "        return records[0] if records else None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch Airtable record: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def ensure_data_format(data):\n",
    "    \"\"\"Ensure data entries have consistent format.\"\"\"\n",
    "    formatted_data = []\n",
    "    for entry in data:\n",
    "        try:\n",
    "            formatted_entry = entry.copy()\n",
    "            if isinstance(formatted_entry.get(\"timestamp\"), datetime):\n",
    "                formatted_entry[\"timestamp\"] = formatted_entry[\"timestamp\"].isoformat()\n",
    "            for key in [\"high_24h_usd\", \"low_24h_usd\", \"volatility_24h_%\", \"trading_range_24h_usd\"]:\n",
    "                if key in formatted_entry:\n",
    "                    formatted_entry[key] = round(float(formatted_entry[key]), 2)\n",
    "            formatted_data.append(formatted_entry)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping malformed data entry: {str(e)}\")\n",
    "    return formatted_data\n",
    "\n",
    "def merge_unique_data(existing_data, new_data):\n",
    "    \"\"\"Merge new data with existing, keeping only unique entries.\"\"\"\n",
    "    existing_set = {(d[\"symbol\"], d[\"timestamp\"]) for d in existing_data}\n",
    "    unique_new = [d for d in new_data if (d[\"symbol\"], d[\"timestamp\"]) not in existing_set]\n",
    "    logger.info(f\"Found {len(unique_new)} unique new entries out of {len(new_data)}\")\n",
    "    return existing_data + unique_new\n",
    "\n",
    "def update_or_create_airtable(record_id, data, raw_url, filename):\n",
    "    \"\"\"Update or create Airtable record.\"\"\"\n",
    "    payload = {\n",
    "        \"fields\": {\n",
    "            \"Name\": \"365-Day Volatility and Range\",\n",
    "            \"Data\": data,\n",
    "            \"Database Attachment\": [{\"url\": raw_url, \"filename\": filename}],\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        if record_id:\n",
    "            logger.info(f\"Updating existing Airtable record: {record_id}\")\n",
    "            update_airtable(record_id, raw_url, filename, AIRTABLE_URL, AIRTABLE_API_KEY)\n",
    "            # Update Data field separately to ensure all fields are updated\n",
    "            patch_url = f\"{AIRTABLE_URL}/{record_id}\"\n",
    "            patch_payload = {\"fields\": {\"Data\": data, \"Name\": \"365-Day Volatility and Range\"}}\n",
    "            response = requests.patch(patch_url, headers=AIRTABLE_HEADERS, json=patch_payload)\n",
    "        else:\n",
    "            logger.info(\"Creating new Airtable record\")\n",
    "            create_airtable_record(raw_url, filename, AIRTABLE_URL, AIRTABLE_API_KEY)\n",
    "            # Update Data field for new record\n",
    "            response = requests.get(AIRTABLE_URL, headers=AIRTABLE_HEADERS, params={\n",
    "                \"filterByFormula\": \"{Name}='365-Day Volatility and Range'\"\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            record_id = response.json()[\"records\"][0][\"id\"]\n",
    "            patch_url = f\"{AIRTABLE_URL}/{record_id}\"\n",
    "            patch_payload = {\"fields\": {\"Data\": data}}\n",
    "            response = requests.patch(patch_url, headers=AIRTABLE_HEADERS, json=patch_payload)\n",
    "        response.raise_for_status()\n",
    "        logger.info(\"Successfully updated/created Airtable record\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Airtable operation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    filename = None\n",
    "    try:\n",
    "        # Fetch data\n",
    "        new_data = fetch_coingecko_data()\n",
    "        if not new_data:\n",
    "            logger.error(\"No data fetched from CoinGecko\")\n",
    "            return\n",
    "\n",
    "        # Create DataFrame and Excel file\n",
    "        df = pd.DataFrame(new_data)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"historical_volatility_trading_range_365_days_{timestamp}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        logger.info(f\"Created Excel file: {filename}\")\n",
    "\n",
    "        # Get existing Airtable record\n",
    "        record = get_airtable_record()\n",
    "        record_id = record[\"id\"] if record else None\n",
    "        existing_data = ensure_data_format(record[\"fields\"].get(\"Data\", []) if record else [])\n",
    "\n",
    "        # Merge unique data\n",
    "        merged_data = merge_unique_data(existing_data, new_data)\n",
    "\n",
    "        # Upload to GitHub\n",
    "        github_response = upload_to_github(filename, GITHUB_REPO, BRANCH, UPLOAD_PATH, GITHUB_TOKEN)\n",
    "        raw_url = github_response[\"content\"][\"download_url\"]\n",
    "        file_sha = github_response[\"content\"][\"sha\"]\n",
    "        logger.info(\"Uploaded file to GitHub\")\n",
    "\n",
    "        # Update or create Airtable record\n",
    "        update_or_create_airtable(record_id, merged_data, raw_url, filename)\n",
    "\n",
    "        # Cleanup\n",
    "        delete_file_from_github(filename, GITHUB_REPO, BRANCH, UPLOAD_PATH, GITHUB_TOKEN, file_sha)\n",
    "        os.remove(filename)\n",
    "        logger.info(\"Cleaned up local and GitHub files\")\n",
    "        filename = None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script failed: {str(e)}\")\n",
    "        if filename and os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "            logger.info(\"Removed local file due to error\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
